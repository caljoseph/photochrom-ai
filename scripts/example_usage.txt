# Basic run for UNet
bash scripts/run_slurm.sh unet

# Run with overrides (Hydra-style)
bash scripts/run_slurm.sh unet model.lr=0.001 data.batch_size=32

# Run a sweep over learning rates
for lr in 0.001 0.0005 0.0001; do
  bash scripts/run_slurm.sh unet model.lr=$lr logger.name=unet_lr_$lr
done